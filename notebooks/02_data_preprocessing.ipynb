{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T14:01:42.326400Z",
     "start_time": "2025-06-05T14:01:37.858790Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Preprocessing des Données - Détection Nodules Pulmonaires\n",
    "# Conversion YOLO → COCO Format pour Faster R-CNN\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from pathlib import Path\n",
    "import random\n",
    "from PIL import Image\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# Configuration\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"✅ Imports preprocessing réussis!\")\n",
    "print(f\"🔥 PyTorch version: {torch.__version__}\")\n",
    "print(f\"🔥 CUDA disponible: {torch.cuda.is_available()}\")"
   ],
   "id": "677c71aa3a1035c7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Imports preprocessing réussis!\n",
      "🔥 PyTorch version: 2.7.0\n",
      "🔥 CUDA disponible: False\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T14:03:08.456499Z",
     "start_time": "2025-06-05T14:03:08.445988Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Configuration des chemins (réutiliser du notebook 01)\n",
    "BASE_PATH = Path(\"../data/raw/ct_images\")\n",
    "TRAIN_IMAGES = BASE_PATH / \"images\" / \"train\"\n",
    "TRAIN_LABELS = BASE_PATH / \"labels\" / \"train\"\n",
    "VAL_IMAGES = BASE_PATH / \"images\" / \"val\"\n",
    "VAL_LABELS = BASE_PATH / \"labels\" / \"val\"\n",
    "\n",
    "# Nouveau dossier pour les données preprocessées\n",
    "PROCESSED_PATH = Path(\"../data/processed\")\n",
    "PROCESSED_PATH.mkdir(exist_ok=True)\n",
    "\n",
    "# Configuration pour le modèle\n",
    "CONFIG = {\n",
    "    'image_size': (512, 512),  # Resize pour Faster R-CNN\n",
    "    'num_classes': 2,          # 0=background, 1=nodule\n",
    "    'batch_size': 4,           # Petit batch pour CUDA False\n",
    "    'normalize_mean': [0.485, 0.456, 0.406],  # ImageNet standard\n",
    "    'normalize_std': [0.229, 0.224, 0.225]\n",
    "}\n",
    "\n",
    "print(\"✅ Configuration créée!\")\n",
    "print(f\"📁 Dossier processed: {PROCESSED_PATH.exists()}\")\n",
    "print(f\"🖼️ Taille images: {CONFIG['image_size']}\")\n",
    "print(f\"⚡ Batch size: {CONFIG['batch_size']} (adapté pour CPU)\")"
   ],
   "id": "eb1eddd391b8b120",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Configuration créée!\n",
      "📁 Dossier processed: True\n",
      "🖼️ Taille images: (512, 512)\n",
      "⚡ Batch size: 4 (adapté pour CPU)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T14:03:42.515893Z",
     "start_time": "2025-06-05T14:03:42.480451Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def yolo_to_coco_format(yolo_boxes, img_width, img_height):\n",
    "    \"\"\"\n",
    "    Convertir les bounding boxes YOLO en format COCO pour Faster R-CNN\n",
    "\n",
    "    YOLO: [class_id, center_x, center_y, width, height] (normalisé 0-1)\n",
    "    COCO: [x1, y1, x2, y2] (coordonnées absolues) + labels séparés\n",
    "    \"\"\"\n",
    "    if not yolo_boxes:\n",
    "        return {\n",
    "            'boxes': torch.zeros((0, 4), dtype=torch.float32),\n",
    "            'labels': torch.zeros((0,), dtype=torch.int64)\n",
    "        }\n",
    "\n",
    "    coco_boxes = []\n",
    "    labels = []\n",
    "\n",
    "    for box in yolo_boxes:\n",
    "        # Convertir coordonnées normalisées → absolues\n",
    "        center_x = box['center_x'] * img_width\n",
    "        center_y = box['center_y'] * img_height\n",
    "        width = box['width'] * img_width\n",
    "        height = box['height'] * img_height\n",
    "\n",
    "        # Calculer coins (x1, y1, x2, y2)\n",
    "        x1 = center_x - width/2\n",
    "        y1 = center_y - height/2\n",
    "        x2 = center_x + width/2\n",
    "        y2 = center_y + height/2\n",
    "\n",
    "        # Clamper les coordonnées dans les limites de l'image\n",
    "        x1 = max(0, min(x1, img_width))\n",
    "        y1 = max(0, min(y1, img_height))\n",
    "        x2 = max(0, min(x2, img_width))\n",
    "        y2 = max(0, min(y2, img_height))\n",
    "\n",
    "        # Vérifier que la box est valide\n",
    "        if x2 > x1 and y2 > y1:\n",
    "            coco_boxes.append([x1, y1, x2, y2])\n",
    "            labels.append(1)  # 1 = nodule (0 sera pour background)\n",
    "\n",
    "    return {\n",
    "        'boxes': torch.tensor(coco_boxes, dtype=torch.float32),\n",
    "        'labels': torch.tensor(labels, dtype=torch.int64)\n",
    "    }\n",
    "\n",
    "# Test de la fonction\n",
    "print(\"🧪 Test de conversion YOLO → COCO:\")\n",
    "test_box = [{'class_id': 1, 'center_x': 0.5, 'center_y': 0.5, 'width': 0.1, 'height': 0.1}]\n",
    "result = yolo_to_coco_format(test_box, 512, 512)\n",
    "print(f\"Input YOLO: {test_box[0]}\")\n",
    "print(f\"Output COCO: boxes={result['boxes']}, labels={result['labels']}\")"
   ],
   "id": "b98b00420751fad6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Test de conversion YOLO → COCO:\n",
      "Input YOLO: {'class_id': 1, 'center_x': 0.5, 'center_y': 0.5, 'width': 0.1, 'height': 0.1}\n",
      "Output COCO: boxes=tensor([[230.4000, 230.4000, 281.6000, 281.6000]]), labels=tensor([1])\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T14:07:50.860351Z",
     "start_time": "2025-06-05T14:07:50.672490Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Fonction pour lire labels YOLO (du notebook 01)\n",
    "def read_yolo_label(label_path):\n",
    "    \"\"\"Réutiliser la fonction du notebook 01\"\"\"\n",
    "    boxes = []\n",
    "    if not label_path.exists():\n",
    "        return boxes\n",
    "\n",
    "    with open(label_path, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                parts = line.split()\n",
    "                boxes.append({\n",
    "                    'class_id': int(parts[0]),\n",
    "                    'center_x': float(parts[1]),\n",
    "                    'center_y': float(parts[2]),\n",
    "                    'width': float(parts[3]),\n",
    "                    'height': float(parts[4])\n",
    "                })\n",
    "    return boxes\n",
    "\n",
    "class LungNoduleDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset custom pour les nodules pulmonaires\n",
    "    Compatible avec Faster R-CNN\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, images_dir, labels_dir, transforms=None, config=CONFIG):\n",
    "        self.images_dir = Path(images_dir)\n",
    "        self.labels_dir = Path(labels_dir)\n",
    "        self.transforms = transforms\n",
    "        self.config = config\n",
    "\n",
    "        # Lister toutes les images\n",
    "        self.image_files = list(self.images_dir.glob('*.jpg'))\n",
    "        print(f\"📊 Dataset créé avec {len(self.image_files)} images\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Charger l'image\n",
    "        img_path = self.image_files[idx]\n",
    "        image = cv2.imread(str(img_path))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Charger les labels\n",
    "        label_name = img_path.stem + '.txt'\n",
    "        label_path = self.labels_dir / label_name\n",
    "        yolo_boxes = read_yolo_label(label_path)\n",
    "\n",
    "        # Dimensions originales\n",
    "        original_height, original_width = image.shape[:2]\n",
    "\n",
    "        # Resize l'image\n",
    "        target_height, target_width = self.config['image_size']\n",
    "        image_resized = cv2.resize(image, (target_width, target_height))\n",
    "\n",
    "        # Ajuster les coordonnées des boxes pour la nouvelle taille\n",
    "        scale_x = target_width / original_width\n",
    "        scale_y = target_height / original_height\n",
    "\n",
    "        adjusted_boxes = []\n",
    "        for box in yolo_boxes:\n",
    "            adjusted_boxes.append({\n",
    "                'class_id': box['class_id'],\n",
    "                'center_x': box['center_x'],  # Déjà normalisé\n",
    "                'center_y': box['center_y'],  # Déjà normalisé\n",
    "                'width': box['width'],        # Déjà normalisé\n",
    "                'height': box['height']       # Déjà normalisé\n",
    "            })\n",
    "\n",
    "        # Convertir en format COCO\n",
    "        target = yolo_to_coco_format(adjusted_boxes, target_width, target_height)\n",
    "\n",
    "        # Convertir image en tensor\n",
    "        image_tensor = torch.from_numpy(image_resized).permute(2, 0, 1).float() / 255.0\n",
    "\n",
    "        # Normalisation ImageNet\n",
    "        normalize = transforms.Normalize(\n",
    "            mean=self.config['normalize_mean'],\n",
    "            std=self.config['normalize_std']\n",
    "        )\n",
    "        image_tensor = normalize(image_tensor)\n",
    "\n",
    "        return image_tensor, target\n",
    "\n",
    "# Test du dataset\n",
    "print(\"🧪 Test du dataset:\")\n",
    "train_dataset = LungNoduleDataset(TRAIN_IMAGES, TRAIN_LABELS)\n",
    "print(f\"✅ Dataset train créé: {len(train_dataset)} échantillons\")\n",
    "\n",
    "# Test d'un échantillon\n",
    "sample_image, sample_target = train_dataset[0]\n",
    "print(f\"📊 Échantillon 0:\")\n",
    "print(f\"  Image shape: {sample_image.shape}\")\n",
    "print(f\"  Boxes: {sample_target['boxes'].shape}\")\n",
    "print(f\"  Labels: {sample_target['labels']}\")"
   ],
   "id": "b12eca6c1ebf7e90",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Test du dataset:\n",
      "📊 Dataset créé avec 239 images\n",
      "✅ Dataset train créé: 239 échantillons\n",
      "📊 Échantillon 0:\n",
      "  Image shape: torch.Size([3, 512, 512])\n",
      "  Boxes: torch.Size([1, 4])\n",
      "  Labels: tensor([1])\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T08:00:31.283308Z",
     "start_time": "2025-06-06T08:00:31.248715Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Définir les transformations d'augmentation CORRIGÉES\n",
    "def get_training_transforms():\n",
    "    \"\"\"\n",
    "    Augmentations pour améliorer la robustesse du modèle\n",
    "    Attention: il faut aussi transformer les bounding boxes !\n",
    "    \"\"\"\n",
    "    return A.Compose([\n",
    "        # Transformations géométriques\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.3),\n",
    "        A.Rotate(limit=15, p=0.5),\n",
    "\n",
    "        # Transformations d'intensité (importantes pour les images médicales)\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "        A.GaussianBlur(blur_limit=3, p=0.3),\n",
    "        # A.GaussNoise retiré - paramètre non supporté dans cette version\n",
    "\n",
    "        # Normalisation\n",
    "        A.Normalize(mean=CONFIG['normalize_mean'], std=CONFIG['normalize_std']),\n",
    "        ToTensorV2()\n",
    "    ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']))\n",
    "\n",
    "def get_validation_transforms():\n",
    "    \"\"\"\n",
    "    Transformations pour validation (seulement normalisation)\n",
    "    \"\"\"\n",
    "    return A.Compose([\n",
    "        A.Normalize(mean=CONFIG['normalize_mean'], std=CONFIG['normalize_std']),\n",
    "        ToTensorV2()\n",
    "    ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']))\n",
    "\n",
    "class LungNoduleDatasetAugmented(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset avec augmentation automatique\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, images_dir, labels_dir, transforms=None, config=CONFIG):\n",
    "        self.images_dir = Path(images_dir)\n",
    "        self.labels_dir = Path(labels_dir)\n",
    "        self.transforms = transforms\n",
    "        self.config = config\n",
    "        self.image_files = list(self.images_dir.glob('*.jpg'))\n",
    "\n",
    "        print(f\"📊 Dataset augmenté créé avec {len(self.image_files)} images\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Charger image et labels\n",
    "        img_path = self.image_files[idx]\n",
    "        image = cv2.imread(str(img_path))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        label_name = img_path.stem + '.txt'\n",
    "        label_path = self.labels_dir / label_name\n",
    "        yolo_boxes = read_yolo_label(label_path)\n",
    "\n",
    "        # Resize\n",
    "        original_height, original_width = image.shape[:2]\n",
    "        target_height, target_width = self.config['image_size']\n",
    "        image = cv2.resize(image, (target_width, target_height))\n",
    "\n",
    "        # Convertir YOLO → Pascal VOC pour Albumentations\n",
    "        bboxes = []\n",
    "        labels = []\n",
    "\n",
    "        for box in yolo_boxes:\n",
    "            # Convertir coordonnées normalisées → absolues\n",
    "            center_x = box['center_x'] * target_width\n",
    "            center_y = box['center_y'] * target_height\n",
    "            width = box['width'] * target_width\n",
    "            height = box['height'] * target_height\n",
    "\n",
    "            x1 = center_x - width/2\n",
    "            y1 = center_y - height/2\n",
    "            x2 = center_x + width/2\n",
    "            y2 = center_y + height/2\n",
    "\n",
    "            # Clamper\n",
    "            x1 = max(0, min(x1, target_width))\n",
    "            y1 = max(0, min(y1, target_height))\n",
    "            x2 = max(0, min(x2, target_width))\n",
    "            y2 = max(0, min(y2, target_height))\n",
    "\n",
    "            if x2 > x1 and y2 > y1:\n",
    "                bboxes.append([x1, y1, x2, y2])\n",
    "                labels.append(1)  # Integer direct\n",
    "\n",
    "        # Appliquer les transformations\n",
    "        if self.transforms:\n",
    "            transformed = self.transforms(image=image, bboxes=bboxes, labels=labels)\n",
    "            image = transformed['image']\n",
    "            bboxes = transformed['bboxes']\n",
    "            labels = transformed['labels']\n",
    "\n",
    "        # Convertir en tensors PyTorch (FIX conversion explicite)\n",
    "        if len(bboxes) > 0:\n",
    "            boxes_tensor = torch.tensor(bboxes, dtype=torch.float32)\n",
    "            labels_tensor = torch.tensor([int(label) for label in labels], dtype=torch.int64)  # FIX\n",
    "        else:\n",
    "            boxes_tensor = torch.zeros((0, 4), dtype=torch.float32)\n",
    "            labels_tensor = torch.zeros((0,), dtype=torch.int64)\n",
    "\n",
    "        target = {\n",
    "            'boxes': boxes_tensor,\n",
    "            'labels': labels_tensor\n",
    "        }\n",
    "\n",
    "        return image, target\n",
    "\n",
    "# Test avec augmentation CORRIGÉ\n",
    "print(\"🔄 Test du dataset avec augmentation CORRIGÉ:\")\n",
    "train_transforms = get_training_transforms()\n",
    "train_dataset_aug = LungNoduleDatasetAugmented(TRAIN_IMAGES, TRAIN_LABELS, train_transforms)\n",
    "\n",
    "# Test plusieurs échantillons pour voir la variation\n",
    "print(\"📊 Test de 3 échantillons augmentés:\")\n",
    "for i in range(3):\n",
    "    img, target = train_dataset_aug[0]  # Même image, augmentations différentes\n",
    "    print(f\"  Échantillon {i+1}: Image {img.shape}, Boxes {target['boxes'].shape}\")\n",
    "\n"
   ],
   "id": "358dacd452c19c82",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Test du dataset avec augmentation CORRIGÉ:\n",
      "📊 Dataset augmenté créé avec 239 images\n",
      "📊 Test de 3 échantillons augmentés:\n",
      "  Échantillon 1: Image torch.Size([3, 512, 512]), Boxes torch.Size([1, 4])\n",
      "  Échantillon 2: Image torch.Size([3, 512, 512]), Boxes torch.Size([1, 4])\n",
      "  Échantillon 3: Image torch.Size([3, 512, 512]), Boxes torch.Size([1, 4])\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T08:04:28.301668Z",
     "start_time": "2025-06-06T08:04:28.266377Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Fonction collate custom pour Faster R-CNN\n",
    "    Chaque image peut avoir un nombre différent de bounding boxes\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    targets = []\n",
    "\n",
    "    for image, target in batch:\n",
    "        images.append(image)\n",
    "        targets.append(target)\n",
    "\n",
    "    return images, targets\n",
    "\n",
    "# Créer les datasets finaux\n",
    "print(\"🔧 Création des datasets finaux...\")\n",
    "\n",
    "# Transformations\n",
    "train_transforms = get_training_transforms()\n",
    "val_transforms = get_validation_transforms()\n",
    "\n",
    "# Datasets\n",
    "train_dataset = LungNoduleDatasetAugmented(TRAIN_IMAGES, TRAIN_LABELS, train_transforms)\n",
    "val_dataset = LungNoduleDatasetAugmented(VAL_IMAGES, VAL_LABELS, val_transforms)\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=True,                    # Mélanger pour l'entraînement\n",
    "    num_workers=0,                   # 0 pour éviter les problèmes multiprocessing\n",
    "    collate_fn=collate_fn           # Function custom pour gérer les boxes variables\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=False,                   # Pas de mélange pour validation\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "print(f\"✅ DataLoaders créés:\")\n",
    "print(f\"   🔹 Train: {len(train_loader)} batches de {CONFIG['batch_size']}\")\n",
    "print(f\"   🔹 Val: {len(val_loader)} batches de {CONFIG['batch_size']}\")\n",
    "\n",
    "# Test d'un batch\n",
    "print(\"\\n🧪 Test d'un batch:\")\n",
    "train_iter = iter(train_loader)\n",
    "sample_images, sample_targets = next(train_iter)\n",
    "\n",
    "print(f\"📊 Batch info:\")\n",
    "print(f\"   🖼️ Images: {len(sample_images)} images de shape {sample_images[0].shape}\")\n",
    "print(f\"   📦 Targets: {len(sample_targets)} dictionnaires\")\n",
    "print(f\"   📊 Premier target: boxes {sample_targets[0]['boxes'].shape}, labels {sample_targets[0]['labels']}\")\n",
    "\n",
    "# Vérification mémoire\n",
    "print(f\"\\n💾 Statistiques:\")\n",
    "print(f\"   📊 Total échantillons train: {len(train_dataset)}\")\n",
    "print(f\"   📊 Total échantillons val: {len(val_dataset)}\")\n",
    "print(f\"   ⚡ Batch size: {CONFIG['batch_size']} (adapté pour CPU)\")\n",
    "print(f\"   🔄 Train batches: {len(train_loader)}\")\n",
    "print(f\"   🔄 Val batches: {len(val_loader)}\")"
   ],
   "id": "7b0c024918a856c1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Création des datasets finaux...\n",
      "📊 Dataset augmenté créé avec 239 images\n",
      "📊 Dataset augmenté créé avec 41 images\n",
      "✅ DataLoaders créés:\n",
      "   🔹 Train: 60 batches de 4\n",
      "   🔹 Val: 11 batches de 4\n",
      "\n",
      "🧪 Test d'un batch:\n",
      "📊 Batch info:\n",
      "   🖼️ Images: 4 images de shape torch.Size([3, 512, 512])\n",
      "   📦 Targets: 4 dictionnaires\n",
      "   📊 Premier target: boxes torch.Size([1, 4]), labels tensor([1])\n",
      "\n",
      "💾 Statistiques:\n",
      "   📊 Total échantillons train: 239\n",
      "   📊 Total échantillons val: 41\n",
      "   ⚡ Batch size: 4 (adapté pour CPU)\n",
      "   🔄 Train batches: 60\n",
      "   🔄 Val batches: 11\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T08:05:53.871176Z",
     "start_time": "2025-06-06T08:04:54.068316Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "def create_model(num_classes):\n",
    "    \"\"\"\n",
    "    Créer Faster R-CNN pré-entraîné sur COCO\n",
    "    Modifier la tête de classification pour nos classes\n",
    "    \"\"\"\n",
    "    # Charger le modèle pré-entraîné\n",
    "    model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "    # Remplacer la tête de classification\n",
    "    # Le modèle COCO a 91 classes, nous en avons 2 (background + nodule)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    return model\n",
    "\n",
    "# Créer le modèle\n",
    "print(\"🤖 Création du modèle Faster R-CNN...\")\n",
    "model = create_model(CONFIG['num_classes'])\n",
    "\n",
    "# Informations sur le modèle\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"✅ Modèle créé:\")\n",
    "print(f\"   🧠 Architecture: Faster R-CNN + ResNet50 + FPN\")\n",
    "print(f\"   📊 Paramètres totaux: {total_params:,}\")\n",
    "print(f\"   🔥 Paramètres entraînables: {trainable_params:,}\")\n",
    "print(f\"   🎯 Classes: {CONFIG['num_classes']} (background + nodule)\")\n",
    "\n",
    "# Test du modèle\n",
    "print(\"\\n🧪 Test du modèle:\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Prendre un batch de test\n",
    "    test_images, test_targets = next(iter(train_loader))\n",
    "\n",
    "    # Faire une prédiction\n",
    "    predictions = model(test_images)\n",
    "\n",
    "    print(f\"📊 Input: {len(test_images)} images\")\n",
    "    print(f\"📊 Output: {len(predictions)} prédictions\")\n",
    "    print(f\"📊 Première prédiction: {len(predictions[0]['boxes'])} boxes détectées\")\n",
    "\n",
    "# Configuration de l'entraînement\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "print(f\"\\n⚡ Device: {device}\")\n",
    "print(f\"🚀 Modèle prêt pour l'entraînement!\")"
   ],
   "id": "23b1db94b45bb519",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 Création du modèle Faster R-CNN...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abchatealiibrahim/PycharmProjects/ai-lung-nodule-detection-fastrcnn/.venv/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/abchatealiibrahim/PycharmProjects/ai-lung-nodule-detection-fastrcnn/.venv/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /Users/abchatealiibrahim/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 160M/160M [00:52<00:00, 3.18MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Modèle créé:\n",
      "   🧠 Architecture: Faster R-CNN + ResNet50 + FPN\n",
      "   📊 Paramètres totaux: 41,299,161\n",
      "   🔥 Paramètres entraînables: 41,076,761\n",
      "   🎯 Classes: 2 (background + nodule)\n",
      "\n",
      "🧪 Test du modèle:\n",
      "📊 Input: 4 images\n",
      "📊 Output: 4 prédictions\n",
      "📊 Première prédiction: 100 boxes détectées\n",
      "\n",
      "⚡ Device: cpu\n",
      "🚀 Modèle prêt pour l'entraînement!\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e2a5d3f5ea7bc351"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
