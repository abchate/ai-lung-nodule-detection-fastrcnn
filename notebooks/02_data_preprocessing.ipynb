{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T14:01:42.326400Z",
     "start_time": "2025-06-05T14:01:37.858790Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Preprocessing des DonnÃ©es - DÃ©tection Nodules Pulmonaires\n",
    "# Conversion YOLO â†’ COCO Format pour Faster R-CNN\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from pathlib import Path\n",
    "import random\n",
    "from PIL import Image\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# Configuration\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"âœ… Imports preprocessing rÃ©ussis!\")\n",
    "print(f\"ğŸ”¥ PyTorch version: {torch.__version__}\")\n",
    "print(f\"ğŸ”¥ CUDA disponible: {torch.cuda.is_available()}\")"
   ],
   "id": "677c71aa3a1035c7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Imports preprocessing rÃ©ussis!\n",
      "ğŸ”¥ PyTorch version: 2.7.0\n",
      "ğŸ”¥ CUDA disponible: False\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T14:03:08.456499Z",
     "start_time": "2025-06-05T14:03:08.445988Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Configuration des chemins (rÃ©utiliser du notebook 01)\n",
    "BASE_PATH = Path(\"../data/raw/ct_images\")\n",
    "TRAIN_IMAGES = BASE_PATH / \"images\" / \"train\"\n",
    "TRAIN_LABELS = BASE_PATH / \"labels\" / \"train\"\n",
    "VAL_IMAGES = BASE_PATH / \"images\" / \"val\"\n",
    "VAL_LABELS = BASE_PATH / \"labels\" / \"val\"\n",
    "\n",
    "# Nouveau dossier pour les donnÃ©es preprocessÃ©es\n",
    "PROCESSED_PATH = Path(\"../data/processed\")\n",
    "PROCESSED_PATH.mkdir(exist_ok=True)\n",
    "\n",
    "# Configuration pour le modÃ¨le\n",
    "CONFIG = {\n",
    "    'image_size': (512, 512),  # Resize pour Faster R-CNN\n",
    "    'num_classes': 2,          # 0=background, 1=nodule\n",
    "    'batch_size': 4,           # Petit batch pour CUDA False\n",
    "    'normalize_mean': [0.485, 0.456, 0.406],  # ImageNet standard\n",
    "    'normalize_std': [0.229, 0.224, 0.225]\n",
    "}\n",
    "\n",
    "print(\"âœ… Configuration crÃ©Ã©e!\")\n",
    "print(f\"ğŸ“ Dossier processed: {PROCESSED_PATH.exists()}\")\n",
    "print(f\"ğŸ–¼ï¸ Taille images: {CONFIG['image_size']}\")\n",
    "print(f\"âš¡ Batch size: {CONFIG['batch_size']} (adaptÃ© pour CPU)\")"
   ],
   "id": "eb1eddd391b8b120",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Configuration crÃ©Ã©e!\n",
      "ğŸ“ Dossier processed: True\n",
      "ğŸ–¼ï¸ Taille images: (512, 512)\n",
      "âš¡ Batch size: 4 (adaptÃ© pour CPU)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T14:03:42.515893Z",
     "start_time": "2025-06-05T14:03:42.480451Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def yolo_to_coco_format(yolo_boxes, img_width, img_height):\n",
    "    \"\"\"\n",
    "    Convertir les bounding boxes YOLO en format COCO pour Faster R-CNN\n",
    "\n",
    "    YOLO: [class_id, center_x, center_y, width, height] (normalisÃ© 0-1)\n",
    "    COCO: [x1, y1, x2, y2] (coordonnÃ©es absolues) + labels sÃ©parÃ©s\n",
    "    \"\"\"\n",
    "    if not yolo_boxes:\n",
    "        return {\n",
    "            'boxes': torch.zeros((0, 4), dtype=torch.float32),\n",
    "            'labels': torch.zeros((0,), dtype=torch.int64)\n",
    "        }\n",
    "\n",
    "    coco_boxes = []\n",
    "    labels = []\n",
    "\n",
    "    for box in yolo_boxes:\n",
    "        # Convertir coordonnÃ©es normalisÃ©es â†’ absolues\n",
    "        center_x = box['center_x'] * img_width\n",
    "        center_y = box['center_y'] * img_height\n",
    "        width = box['width'] * img_width\n",
    "        height = box['height'] * img_height\n",
    "\n",
    "        # Calculer coins (x1, y1, x2, y2)\n",
    "        x1 = center_x - width/2\n",
    "        y1 = center_y - height/2\n",
    "        x2 = center_x + width/2\n",
    "        y2 = center_y + height/2\n",
    "\n",
    "        # Clamper les coordonnÃ©es dans les limites de l'image\n",
    "        x1 = max(0, min(x1, img_width))\n",
    "        y1 = max(0, min(y1, img_height))\n",
    "        x2 = max(0, min(x2, img_width))\n",
    "        y2 = max(0, min(y2, img_height))\n",
    "\n",
    "        # VÃ©rifier que la box est valide\n",
    "        if x2 > x1 and y2 > y1:\n",
    "            coco_boxes.append([x1, y1, x2, y2])\n",
    "            labels.append(1)  # 1 = nodule (0 sera pour background)\n",
    "\n",
    "    return {\n",
    "        'boxes': torch.tensor(coco_boxes, dtype=torch.float32),\n",
    "        'labels': torch.tensor(labels, dtype=torch.int64)\n",
    "    }\n",
    "\n",
    "# Test de la fonction\n",
    "print(\"ğŸ§ª Test de conversion YOLO â†’ COCO:\")\n",
    "test_box = [{'class_id': 1, 'center_x': 0.5, 'center_y': 0.5, 'width': 0.1, 'height': 0.1}]\n",
    "result = yolo_to_coco_format(test_box, 512, 512)\n",
    "print(f\"Input YOLO: {test_box[0]}\")\n",
    "print(f\"Output COCO: boxes={result['boxes']}, labels={result['labels']}\")"
   ],
   "id": "b98b00420751fad6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§ª Test de conversion YOLO â†’ COCO:\n",
      "Input YOLO: {'class_id': 1, 'center_x': 0.5, 'center_y': 0.5, 'width': 0.1, 'height': 0.1}\n",
      "Output COCO: boxes=tensor([[230.4000, 230.4000, 281.6000, 281.6000]]), labels=tensor([1])\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T14:07:50.860351Z",
     "start_time": "2025-06-05T14:07:50.672490Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Fonction pour lire labels YOLO (du notebook 01)\n",
    "def read_yolo_label(label_path):\n",
    "    \"\"\"RÃ©utiliser la fonction du notebook 01\"\"\"\n",
    "    boxes = []\n",
    "    if not label_path.exists():\n",
    "        return boxes\n",
    "\n",
    "    with open(label_path, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                parts = line.split()\n",
    "                boxes.append({\n",
    "                    'class_id': int(parts[0]),\n",
    "                    'center_x': float(parts[1]),\n",
    "                    'center_y': float(parts[2]),\n",
    "                    'width': float(parts[3]),\n",
    "                    'height': float(parts[4])\n",
    "                })\n",
    "    return boxes\n",
    "\n",
    "class LungNoduleDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset custom pour les nodules pulmonaires\n",
    "    Compatible avec Faster R-CNN\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, images_dir, labels_dir, transforms=None, config=CONFIG):\n",
    "        self.images_dir = Path(images_dir)\n",
    "        self.labels_dir = Path(labels_dir)\n",
    "        self.transforms = transforms\n",
    "        self.config = config\n",
    "\n",
    "        # Lister toutes les images\n",
    "        self.image_files = list(self.images_dir.glob('*.jpg'))\n",
    "        print(f\"ğŸ“Š Dataset crÃ©Ã© avec {len(self.image_files)} images\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Charger l'image\n",
    "        img_path = self.image_files[idx]\n",
    "        image = cv2.imread(str(img_path))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Charger les labels\n",
    "        label_name = img_path.stem + '.txt'\n",
    "        label_path = self.labels_dir / label_name\n",
    "        yolo_boxes = read_yolo_label(label_path)\n",
    "\n",
    "        # Dimensions originales\n",
    "        original_height, original_width = image.shape[:2]\n",
    "\n",
    "        # Resize l'image\n",
    "        target_height, target_width = self.config['image_size']\n",
    "        image_resized = cv2.resize(image, (target_width, target_height))\n",
    "\n",
    "        # Ajuster les coordonnÃ©es des boxes pour la nouvelle taille\n",
    "        scale_x = target_width / original_width\n",
    "        scale_y = target_height / original_height\n",
    "\n",
    "        adjusted_boxes = []\n",
    "        for box in yolo_boxes:\n",
    "            adjusted_boxes.append({\n",
    "                'class_id': box['class_id'],\n",
    "                'center_x': box['center_x'],  # DÃ©jÃ  normalisÃ©\n",
    "                'center_y': box['center_y'],  # DÃ©jÃ  normalisÃ©\n",
    "                'width': box['width'],        # DÃ©jÃ  normalisÃ©\n",
    "                'height': box['height']       # DÃ©jÃ  normalisÃ©\n",
    "            })\n",
    "\n",
    "        # Convertir en format COCO\n",
    "        target = yolo_to_coco_format(adjusted_boxes, target_width, target_height)\n",
    "\n",
    "        # Convertir image en tensor\n",
    "        image_tensor = torch.from_numpy(image_resized).permute(2, 0, 1).float() / 255.0\n",
    "\n",
    "        # Normalisation ImageNet\n",
    "        normalize = transforms.Normalize(\n",
    "            mean=self.config['normalize_mean'],\n",
    "            std=self.config['normalize_std']\n",
    "        )\n",
    "        image_tensor = normalize(image_tensor)\n",
    "\n",
    "        return image_tensor, target\n",
    "\n",
    "# Test du dataset\n",
    "print(\"ğŸ§ª Test du dataset:\")\n",
    "train_dataset = LungNoduleDataset(TRAIN_IMAGES, TRAIN_LABELS)\n",
    "print(f\"âœ… Dataset train crÃ©Ã©: {len(train_dataset)} Ã©chantillons\")\n",
    "\n",
    "# Test d'un Ã©chantillon\n",
    "sample_image, sample_target = train_dataset[0]\n",
    "print(f\"ğŸ“Š Ã‰chantillon 0:\")\n",
    "print(f\"  Image shape: {sample_image.shape}\")\n",
    "print(f\"  Boxes: {sample_target['boxes'].shape}\")\n",
    "print(f\"  Labels: {sample_target['labels']}\")"
   ],
   "id": "b12eca6c1ebf7e90",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§ª Test du dataset:\n",
      "ğŸ“Š Dataset crÃ©Ã© avec 239 images\n",
      "âœ… Dataset train crÃ©Ã©: 239 Ã©chantillons\n",
      "ğŸ“Š Ã‰chantillon 0:\n",
      "  Image shape: torch.Size([3, 512, 512])\n",
      "  Boxes: torch.Size([1, 4])\n",
      "  Labels: tensor([1])\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T08:00:31.283308Z",
     "start_time": "2025-06-06T08:00:31.248715Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# DÃ©finir les transformations d'augmentation CORRIGÃ‰ES\n",
    "def get_training_transforms():\n",
    "    \"\"\"\n",
    "    Augmentations pour amÃ©liorer la robustesse du modÃ¨le\n",
    "    Attention: il faut aussi transformer les bounding boxes !\n",
    "    \"\"\"\n",
    "    return A.Compose([\n",
    "        # Transformations gÃ©omÃ©triques\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.3),\n",
    "        A.Rotate(limit=15, p=0.5),\n",
    "\n",
    "        # Transformations d'intensitÃ© (importantes pour les images mÃ©dicales)\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "        A.GaussianBlur(blur_limit=3, p=0.3),\n",
    "        # A.GaussNoise retirÃ© - paramÃ¨tre non supportÃ© dans cette version\n",
    "\n",
    "        # Normalisation\n",
    "        A.Normalize(mean=CONFIG['normalize_mean'], std=CONFIG['normalize_std']),\n",
    "        ToTensorV2()\n",
    "    ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']))\n",
    "\n",
    "def get_validation_transforms():\n",
    "    \"\"\"\n",
    "    Transformations pour validation (seulement normalisation)\n",
    "    \"\"\"\n",
    "    return A.Compose([\n",
    "        A.Normalize(mean=CONFIG['normalize_mean'], std=CONFIG['normalize_std']),\n",
    "        ToTensorV2()\n",
    "    ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']))\n",
    "\n",
    "class LungNoduleDatasetAugmented(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset avec augmentation automatique\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, images_dir, labels_dir, transforms=None, config=CONFIG):\n",
    "        self.images_dir = Path(images_dir)\n",
    "        self.labels_dir = Path(labels_dir)\n",
    "        self.transforms = transforms\n",
    "        self.config = config\n",
    "        self.image_files = list(self.images_dir.glob('*.jpg'))\n",
    "\n",
    "        print(f\"ğŸ“Š Dataset augmentÃ© crÃ©Ã© avec {len(self.image_files)} images\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Charger image et labels\n",
    "        img_path = self.image_files[idx]\n",
    "        image = cv2.imread(str(img_path))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        label_name = img_path.stem + '.txt'\n",
    "        label_path = self.labels_dir / label_name\n",
    "        yolo_boxes = read_yolo_label(label_path)\n",
    "\n",
    "        # Resize\n",
    "        original_height, original_width = image.shape[:2]\n",
    "        target_height, target_width = self.config['image_size']\n",
    "        image = cv2.resize(image, (target_width, target_height))\n",
    "\n",
    "        # Convertir YOLO â†’ Pascal VOC pour Albumentations\n",
    "        bboxes = []\n",
    "        labels = []\n",
    "\n",
    "        for box in yolo_boxes:\n",
    "            # Convertir coordonnÃ©es normalisÃ©es â†’ absolues\n",
    "            center_x = box['center_x'] * target_width\n",
    "            center_y = box['center_y'] * target_height\n",
    "            width = box['width'] * target_width\n",
    "            height = box['height'] * target_height\n",
    "\n",
    "            x1 = center_x - width/2\n",
    "            y1 = center_y - height/2\n",
    "            x2 = center_x + width/2\n",
    "            y2 = center_y + height/2\n",
    "\n",
    "            # Clamper\n",
    "            x1 = max(0, min(x1, target_width))\n",
    "            y1 = max(0, min(y1, target_height))\n",
    "            x2 = max(0, min(x2, target_width))\n",
    "            y2 = max(0, min(y2, target_height))\n",
    "\n",
    "            if x2 > x1 and y2 > y1:\n",
    "                bboxes.append([x1, y1, x2, y2])\n",
    "                labels.append(1)  # Integer direct\n",
    "\n",
    "        # Appliquer les transformations\n",
    "        if self.transforms:\n",
    "            transformed = self.transforms(image=image, bboxes=bboxes, labels=labels)\n",
    "            image = transformed['image']\n",
    "            bboxes = transformed['bboxes']\n",
    "            labels = transformed['labels']\n",
    "\n",
    "        # Convertir en tensors PyTorch (FIX conversion explicite)\n",
    "        if len(bboxes) > 0:\n",
    "            boxes_tensor = torch.tensor(bboxes, dtype=torch.float32)\n",
    "            labels_tensor = torch.tensor([int(label) for label in labels], dtype=torch.int64)  # FIX\n",
    "        else:\n",
    "            boxes_tensor = torch.zeros((0, 4), dtype=torch.float32)\n",
    "            labels_tensor = torch.zeros((0,), dtype=torch.int64)\n",
    "\n",
    "        target = {\n",
    "            'boxes': boxes_tensor,\n",
    "            'labels': labels_tensor\n",
    "        }\n",
    "\n",
    "        return image, target\n",
    "\n",
    "# Test avec augmentation CORRIGÃ‰\n",
    "print(\"ğŸ”„ Test du dataset avec augmentation CORRIGÃ‰:\")\n",
    "train_transforms = get_training_transforms()\n",
    "train_dataset_aug = LungNoduleDatasetAugmented(TRAIN_IMAGES, TRAIN_LABELS, train_transforms)\n",
    "\n",
    "# Test plusieurs Ã©chantillons pour voir la variation\n",
    "print(\"ğŸ“Š Test de 3 Ã©chantillons augmentÃ©s:\")\n",
    "for i in range(3):\n",
    "    img, target = train_dataset_aug[0]  # MÃªme image, augmentations diffÃ©rentes\n",
    "    print(f\"  Ã‰chantillon {i+1}: Image {img.shape}, Boxes {target['boxes'].shape}\")\n",
    "\n"
   ],
   "id": "358dacd452c19c82",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Test du dataset avec augmentation CORRIGÃ‰:\n",
      "ğŸ“Š Dataset augmentÃ© crÃ©Ã© avec 239 images\n",
      "ğŸ“Š Test de 3 Ã©chantillons augmentÃ©s:\n",
      "  Ã‰chantillon 1: Image torch.Size([3, 512, 512]), Boxes torch.Size([1, 4])\n",
      "  Ã‰chantillon 2: Image torch.Size([3, 512, 512]), Boxes torch.Size([1, 4])\n",
      "  Ã‰chantillon 3: Image torch.Size([3, 512, 512]), Boxes torch.Size([1, 4])\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T08:04:28.301668Z",
     "start_time": "2025-06-06T08:04:28.266377Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Fonction collate custom pour Faster R-CNN\n",
    "    Chaque image peut avoir un nombre diffÃ©rent de bounding boxes\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    targets = []\n",
    "\n",
    "    for image, target in batch:\n",
    "        images.append(image)\n",
    "        targets.append(target)\n",
    "\n",
    "    return images, targets\n",
    "\n",
    "# CrÃ©er les datasets finaux\n",
    "print(\"ğŸ”§ CrÃ©ation des datasets finaux...\")\n",
    "\n",
    "# Transformations\n",
    "train_transforms = get_training_transforms()\n",
    "val_transforms = get_validation_transforms()\n",
    "\n",
    "# Datasets\n",
    "train_dataset = LungNoduleDatasetAugmented(TRAIN_IMAGES, TRAIN_LABELS, train_transforms)\n",
    "val_dataset = LungNoduleDatasetAugmented(VAL_IMAGES, VAL_LABELS, val_transforms)\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=True,                    # MÃ©langer pour l'entraÃ®nement\n",
    "    num_workers=0,                   # 0 pour Ã©viter les problÃ¨mes multiprocessing\n",
    "    collate_fn=collate_fn           # Function custom pour gÃ©rer les boxes variables\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=False,                   # Pas de mÃ©lange pour validation\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "print(f\"âœ… DataLoaders crÃ©Ã©s:\")\n",
    "print(f\"   ğŸ”¹ Train: {len(train_loader)} batches de {CONFIG['batch_size']}\")\n",
    "print(f\"   ğŸ”¹ Val: {len(val_loader)} batches de {CONFIG['batch_size']}\")\n",
    "\n",
    "# Test d'un batch\n",
    "print(\"\\nğŸ§ª Test d'un batch:\")\n",
    "train_iter = iter(train_loader)\n",
    "sample_images, sample_targets = next(train_iter)\n",
    "\n",
    "print(f\"ğŸ“Š Batch info:\")\n",
    "print(f\"   ğŸ–¼ï¸ Images: {len(sample_images)} images de shape {sample_images[0].shape}\")\n",
    "print(f\"   ğŸ“¦ Targets: {len(sample_targets)} dictionnaires\")\n",
    "print(f\"   ğŸ“Š Premier target: boxes {sample_targets[0]['boxes'].shape}, labels {sample_targets[0]['labels']}\")\n",
    "\n",
    "# VÃ©rification mÃ©moire\n",
    "print(f\"\\nğŸ’¾ Statistiques:\")\n",
    "print(f\"   ğŸ“Š Total Ã©chantillons train: {len(train_dataset)}\")\n",
    "print(f\"   ğŸ“Š Total Ã©chantillons val: {len(val_dataset)}\")\n",
    "print(f\"   âš¡ Batch size: {CONFIG['batch_size']} (adaptÃ© pour CPU)\")\n",
    "print(f\"   ğŸ”„ Train batches: {len(train_loader)}\")\n",
    "print(f\"   ğŸ”„ Val batches: {len(val_loader)}\")"
   ],
   "id": "7b0c024918a856c1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ CrÃ©ation des datasets finaux...\n",
      "ğŸ“Š Dataset augmentÃ© crÃ©Ã© avec 239 images\n",
      "ğŸ“Š Dataset augmentÃ© crÃ©Ã© avec 41 images\n",
      "âœ… DataLoaders crÃ©Ã©s:\n",
      "   ğŸ”¹ Train: 60 batches de 4\n",
      "   ğŸ”¹ Val: 11 batches de 4\n",
      "\n",
      "ğŸ§ª Test d'un batch:\n",
      "ğŸ“Š Batch info:\n",
      "   ğŸ–¼ï¸ Images: 4 images de shape torch.Size([3, 512, 512])\n",
      "   ğŸ“¦ Targets: 4 dictionnaires\n",
      "   ğŸ“Š Premier target: boxes torch.Size([1, 4]), labels tensor([1])\n",
      "\n",
      "ğŸ’¾ Statistiques:\n",
      "   ğŸ“Š Total Ã©chantillons train: 239\n",
      "   ğŸ“Š Total Ã©chantillons val: 41\n",
      "   âš¡ Batch size: 4 (adaptÃ© pour CPU)\n",
      "   ğŸ”„ Train batches: 60\n",
      "   ğŸ”„ Val batches: 11\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T08:05:53.871176Z",
     "start_time": "2025-06-06T08:04:54.068316Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "def create_model(num_classes):\n",
    "    \"\"\"\n",
    "    CrÃ©er Faster R-CNN prÃ©-entraÃ®nÃ© sur COCO\n",
    "    Modifier la tÃªte de classification pour nos classes\n",
    "    \"\"\"\n",
    "    # Charger le modÃ¨le prÃ©-entraÃ®nÃ©\n",
    "    model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "    # Remplacer la tÃªte de classification\n",
    "    # Le modÃ¨le COCO a 91 classes, nous en avons 2 (background + nodule)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    return model\n",
    "\n",
    "# CrÃ©er le modÃ¨le\n",
    "print(\"ğŸ¤– CrÃ©ation du modÃ¨le Faster R-CNN...\")\n",
    "model = create_model(CONFIG['num_classes'])\n",
    "\n",
    "# Informations sur le modÃ¨le\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"âœ… ModÃ¨le crÃ©Ã©:\")\n",
    "print(f\"   ğŸ§  Architecture: Faster R-CNN + ResNet50 + FPN\")\n",
    "print(f\"   ğŸ“Š ParamÃ¨tres totaux: {total_params:,}\")\n",
    "print(f\"   ğŸ”¥ ParamÃ¨tres entraÃ®nables: {trainable_params:,}\")\n",
    "print(f\"   ğŸ¯ Classes: {CONFIG['num_classes']} (background + nodule)\")\n",
    "\n",
    "# Test du modÃ¨le\n",
    "print(\"\\nğŸ§ª Test du modÃ¨le:\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Prendre un batch de test\n",
    "    test_images, test_targets = next(iter(train_loader))\n",
    "\n",
    "    # Faire une prÃ©diction\n",
    "    predictions = model(test_images)\n",
    "\n",
    "    print(f\"ğŸ“Š Input: {len(test_images)} images\")\n",
    "    print(f\"ğŸ“Š Output: {len(predictions)} prÃ©dictions\")\n",
    "    print(f\"ğŸ“Š PremiÃ¨re prÃ©diction: {len(predictions[0]['boxes'])} boxes dÃ©tectÃ©es\")\n",
    "\n",
    "# Configuration de l'entraÃ®nement\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "print(f\"\\nâš¡ Device: {device}\")\n",
    "print(f\"ğŸš€ ModÃ¨le prÃªt pour l'entraÃ®nement!\")"
   ],
   "id": "23b1db94b45bb519",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– CrÃ©ation du modÃ¨le Faster R-CNN...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abchatealiibrahim/PycharmProjects/ai-lung-nodule-detection-fastrcnn/.venv/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/abchatealiibrahim/PycharmProjects/ai-lung-nodule-detection-fastrcnn/.venv/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /Users/abchatealiibrahim/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160M/160M [00:52<00:00, 3.18MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ModÃ¨le crÃ©Ã©:\n",
      "   ğŸ§  Architecture: Faster R-CNN + ResNet50 + FPN\n",
      "   ğŸ“Š ParamÃ¨tres totaux: 41,299,161\n",
      "   ğŸ”¥ ParamÃ¨tres entraÃ®nables: 41,076,761\n",
      "   ğŸ¯ Classes: 2 (background + nodule)\n",
      "\n",
      "ğŸ§ª Test du modÃ¨le:\n",
      "ğŸ“Š Input: 4 images\n",
      "ğŸ“Š Output: 4 prÃ©dictions\n",
      "ğŸ“Š PremiÃ¨re prÃ©diction: 100 boxes dÃ©tectÃ©es\n",
      "\n",
      "âš¡ Device: cpu\n",
      "ğŸš€ ModÃ¨le prÃªt pour l'entraÃ®nement!\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e2a5d3f5ea7bc351"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
